{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "stories_data = []\n",
    "\n",
    "# Loop through each month and day\n",
    "for month in range(1, 4):\n",
    "    if month in [1, 3, 5, 7, 8, 10, 12]:\n",
    "        n_days = 31\n",
    "    elif month in [4, 6, 9, 11]:\n",
    "        n_days = 30\n",
    "    else:\n",
    "        n_days = 28  # February, not accounting for leap years\n",
    "\n",
    "    for day in range(1, n_days + 1):\n",
    "        # Format month and day to two digits\n",
    "        month_str = f'{month:02}'\n",
    "        day_str = f'{day:02}'\n",
    "\n",
    "        date = f'{month_str}/{day_str}/2019'\n",
    "        url = f'https://medium.com/swlh/archive/2019/{month_str}/{day_str}'\n",
    "\n",
    "        # Request the page\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "        # Find all stories\n",
    "        stories = soup.find_all('div', class_='streamItem streamItem--postPreview js-streamItem')\n",
    "        \n",
    "        for story in stories:\n",
    "            each_story = {}\n",
    "\n",
    "            # Find author box\n",
    "            author_box = story.find('div', class_='postMetaInline u-floatLeft u-sm-maxWidthFullWidth')\n",
    "            if author_box:\n",
    "                author_url = author_box.find('a')['href']\n",
    "            else:\n",
    "                author_url = 'N/A'\n",
    "\n",
    "            # Get reading time\n",
    "            try:\n",
    "                reading_time = author_box.find('span', class_='readingTime')['title']\n",
    "            except (TypeError, AttributeError):\n",
    "                reading_time = 'N/A'\n",
    "\n",
    "            # Get title and subtitle\n",
    "            title = story.find('h3').text if story.find('h3') else '-'\n",
    "            subtitle = story.find('h4').text if story.find('h4') else '-'\n",
    "\n",
    "            # Get claps\n",
    "            claps_button = story.find('button', class_='button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents')\n",
    "            claps = claps_button.text if claps_button else '0'\n",
    "\n",
    "            # Get responses\n",
    "            responses_button = story.find('a', class_='button button--chromeless u-baseColor--buttonNormal')\n",
    "            responses = responses_button.text if responses_button else '0 responses'\n",
    "\n",
    "            # Get story URL\n",
    "            story_element = story.find('a', class_='button button--smaller button--chromeless u-baseColor--buttonNormal')\n",
    "            story_url = story_element['href'] if story_element else 'N/A'\n",
    "\n",
    "            # Clean up reading time and responses\n",
    "            reading_time = reading_time.split()[0] if reading_time != 'N/A' else 'N/A'\n",
    "            responses = responses.split()[0] if responses != '0 responses' else '0'\n",
    "\n",
    "            # Store the data in a dictionary\n",
    "            each_story = {\n",
    "                'date': date,\n",
    "                 \n",
    "                'title': title,\n",
    "                'subtitle': subtitle,\n",
    "                'claps': claps,\n",
    "                'responses': responses,\n",
    "                'story_url': story_url\n",
    "            }\n",
    "\n",
    "            # Append the story data to the list\n",
    "            stories_data.append(each_story)\n",
    "\n",
    "# Convert the list of stories to a DataFrame\n",
    "stories_df = pd.DataFrame(stories_data)\n",
    "\n",
    "# Optionally, save to a CSV file\n",
    "stories_df.to_csv('medium_stories_2019.csv', index=False)\n",
    "\n",
    "print(\"Scraping completed. Data saved to medium_stories_2019.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed. Data saved to medium_stories_2019.csv.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed. Data saved to tamil_lines.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = 'https://www.projectmadurai.org/pm_etexts/utf8/pmuni0001.html'  # Replace with the actual URL\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract text from the webpage\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # Regular expression to match Tamil characters\n",
    "    tamil_pattern = re.compile(r'[\\u0B80-\\u0BFF]+')  # Unicode range for Tamil characters\n",
    "\n",
    "    # Find all Tamil letters in the text\n",
    "    tamil_letters = tamil_pattern.findall(text)\n",
    "\n",
    "    # Join the list of Tamil letters into a single string\n",
    "    tamil_text = ' '.join(tamil_letters)\n",
    "\n",
    "    # Print the extracted Tamil text\n",
    "    print(tamil_text)\n",
    "\n",
    "    # Create a DataFrame from the extracted Tamil text\n",
    "    # Convert the string into a list of lines (or sentences) for better CSV formatting\n",
    "    tamil_text_list = tamil_text.splitlines()\n",
    "    stories_df = pd.DataFrame(tamil_text_list, columns=['Tamil Text'])\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    stories_df.to_csv('tamil.csv', index=False)\n",
    "\n",
    "    print(\"Scraping completed. Data saved to tamil.csv.\")\n",
    "else:\n",
    "    print(f'Failed to retrieve data: {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = 'https://www.projectmadurai.org/pm_etexts/utf8/pmuni0001.html'  # Replace with the actual URL\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract text from the webpage\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # Regular expression to match Tamil characters\n",
    "    tamil_pattern = re.compile(r'[\\u0B80-\\u0BFF]+')  # Unicode range for Tamil characters\n",
    "\n",
    "    # Split the text into lines\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    # List to hold Tamil lines\n",
    "    tamil_lines = []\n",
    "\n",
    "    # Iterate through each line and extract Tamil text\n",
    "    for line in lines:\n",
    "        # Find all Tamil letters in the line\n",
    "        tamil_letters = tamil_pattern.findall(line)\n",
    "        if tamil_letters:  # If there are Tamil letters in the line\n",
    "            # Join the found Tamil letters and add to the list\n",
    "            tamil_lines.append(' '.join(tamil_letters))\n",
    "\n",
    "    # Create a DataFrame from the list of Tamil lines\n",
    "    stories_df = pd.DataFrame(tamil_lines, columns=['Tamil Text'])\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    stories_df.to_csv('tamil_lines.csv', index=False)\n",
    "\n",
    "    print(\"Scraping completed. Data saved to tamil_lines.csv.\")\n",
    "else:\n",
    "    print(f'Failed to retrieve data: {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
